{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Installing the torchsummary package for getting summary of network\n!pip install torchsummary","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting torchsummary\n  Downloading https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Importing the required packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as n\nimport torch.nn.functional as f\nimport numpy as np\nimport os\nfrom torchsummary import summary\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom torchvision import models\nimport cv2\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the celebrity dataset folder and listing all images"},{"metadata":{"trusted":true},"cell_type":"code","source":"celebData = \"celeba-dataset/img_align_celeba/img_align_celeba/\"\nimages = os.listdir(celebData)\nimageList = images[:1500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(imageList)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assigning the device as cuda if GPU is available else cpu"},{"metadata":{"trusted":true},"cell_type":"code","source":"cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the generator architecture as given the reference paper"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(n.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = n.Conv2d(3,64,9,padding=4,bias=False)\n        self.conv2 = n.Conv2d(64,64,3,padding=1,bias=False)\n        self.conv3_1 = n.Conv2d(64,256,3,padding=1,bias=False)\n        self.conv3_2 = n.Conv2d(64,256,3,padding=1,bias=False)\n        self.conv4 = n.Conv2d(64,3,9,padding=4,bias=False)\n        self.bn = n.BatchNorm2d(64)\n        self.ps = n.PixelShuffle(2)\n        self.prelu = n.PReLU()\n        \n    def forward(self,x):\n        block1 = self.prelu(self.conv1(x))\n        block2 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block1))))),block1)\n        block3 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block2))))),block2)\n        block4 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block3))))),block3)\n        block5 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block4))))),block4)\n        block6 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block5))))),block5)\n        block7 = torch.add(self.bn(self.conv2(block6)),block1)\n        block8 = self.prelu(self.ps(self.conv3_1(block7)))\n        block9 = self.prelu(self.ps(self.conv3_2(block8)))\n        block10 = self.conv4(block9)\n        return block10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assigning the Generator to cuda(if available) and printing the summary of the Generator with a dummy input"},{"metadata":{"trusted":true},"cell_type":"code","source":"gen = Generator().to(cuda)\n\n#Uncomment below mentioned three lines if you have more than one gpu and want to use all of them\n#ngpu=2\n# if (cuda.type == 'cuda') and (ngpu > 1):\n#     gen = n.DataParallel(gen, list(range(ngpu)))\nsummary(gen,(3,64,64))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the Discriminator network as given in the reference paper expect that a dropout in the last layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Discriminator(n.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = n.Conv2d(3,64,3,padding=1,bias=False)\n        self.conv2 = n.Conv2d(64,64,3,stride=2,padding=1,bias=False)\n        self.bn2 = n.BatchNorm2d(64)\n        self.conv3 = n.Conv2d(64,128,3,padding=1,bias=False)\n        self.bn3 = n.BatchNorm2d(128)\n        self.conv4 = n.Conv2d(128,128,3,stride=2,padding=1,bias=False)\n        self.bn4 = n.BatchNorm2d(128)\n        self.conv5 = n.Conv2d(128,256,3,padding=1,bias=False)\n        self.bn5 = n.BatchNorm2d(256)\n        self.conv6 = n.Conv2d(256,256,3,stride=2,padding=1,bias=False)\n        self.bn6 = n.BatchNorm2d(256)\n        self.conv7 = n.Conv2d(256,512,3,padding=1,bias=False)\n        self.bn7 = n.BatchNorm2d(512)\n        self.conv8 = n.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n        self.bn8 = n.BatchNorm2d(512)\n        self.fc1 = n.Linear(512*16*16,1024)\n        self.fc2 = n.Linear(1024,1)\n        self.drop = n.Dropout2d(0.3)\n        \n    def forward(self,x):\n        block1 = f.leaky_relu(self.conv1(x))\n        block2 = f.leaky_relu(self.bn2(self.conv2(block1)))\n        block3 = f.leaky_relu(self.bn3(self.conv3(block2)))\n        block4 = f.leaky_relu(self.bn4(self.conv4(block3)))\n        block5 = f.leaky_relu(self.bn5(self.conv5(block4)))\n        block6 = f.leaky_relu(self.bn6(self.conv6(block5)))\n        block7 = f.leaky_relu(self.bn7(self.conv7(block6)))\n        block8 = f.leaky_relu(self.bn8(self.conv8(block7)))\n        block8 = block8.view(-1,block8.size(1)*block8.size(2)*block8.size(3))\n        block9 = f.leaky_relu(self.fc1(block8))\n#         block9 = block9.view(-1,block9.size(1)*block9.size(2)*block9.size(3))\n        block10 = torch.sigmoid(self.drop(self.fc2(block9)))\n        return block9,block10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assigning the discriminator to the gpu(if available) and printing the summary of the network with a dummy value"},{"metadata":{"trusted":true},"cell_type":"code","source":"disc = Discriminator().to(cuda)\nsummary(disc,(3,256,256))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disc = Discriminator().to(cuda).float()\ngen = Generator().to(cuda).float()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Downloading the pretrained vgg19 model from model module of torchvision library"},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg = models.vgg19(pretrained=True).to(cuda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the losses to be used in training"},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_loss = n.BCELoss()\nvgg_loss = n.MSELoss()\nmse_loss = n.MSELoss()\ndisc_loss = n.BCELoss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the adam optimizers for generator and discriminator"},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_optimizer = optim.Adam(gen.parameters(),lr=0.0001)\ndisc_optimizer = optim.Adam(disc.parameters(),lr=0.0001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading the images after based on resizing as numpy array"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadImages(imageList,path,resize=False):\n    images=[]\n    for image in (imageList):\n        if resize==True:\n            img = cv2.resize(cv2.imread(os.path.join(path,image)),(256,256)) \n        else:\n            img = cv2.imread(os.path.join(path,image))\n        img = img.reshape(img.shape[2],img.shape[0],img.shape[1])\n        images.append(img)\n    return np.array(images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting the high resolution images into low resolution by applying gaussian blur, resizing in to 64*64 and loading it as numpy array"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadLRImages(imagelist,path):\n    images=[]\n    for image in (imagelist):\n        img = cv2.resize(cv2.GaussianBlur(cv2.imread(os.path.join(path,image)),(5,5),cv2.BORDER_DEFAULT),(64,64)) \n        img = img.reshape(img.shape[2],img.shape[0],img.shape[1])\n        images.append(img)\n    return np.array(images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading the generator model from the given checkpoint"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n    \n    model.eval()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given low resolution images and checkpoint, Generating the high resolution images out of it "},{"metadata":{"trusted":true},"cell_type":"code","source":"def imagePostProcess(imagedir,modelPath):\n    imagelist=[]\n#     images = os.listdir(imagedir)\n    for img in imagedir:\n        img = os.path.join(celebData,img)\n#         print(img)\n        img = cv2.imread(img)\n        imagelist.append(img)\n    imagearray = np.array(imagelist)/255\n#     imagearray = (imagedir)/255\n    imagearrayPT = np.reshape(imagearray,(len(imagelist),imagearray.shape[3],imagearray.shape[1],imagearray.shape[2]))\n    model = load_checkpoint(modelPath)\n    im_tensor = torch.from_numpy(imagearrayPT).float()\n    out_tensor = model(im_tensor)\n#     print(out_tensor.shape)\n    out = np.reshape(out_tensor,[out_tensor.shape[0],out_tensor.shape[2],out_tensor.shape[3],out_tensor.shape[1]])\n    out = out.numpy()\n    \n    out = np.clip(out,0,1)\n    \n    return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Display utility of displaying images using matplotlib"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_samples(sample_images):\n    figure, axes = plt.subplots(1, sample_images.shape[0], figsize = (10,10))\n    for index, axis in enumerate(axes):\n        axis.axis('off')\n        image_array = sample_images[index]\n        axis.imshow(image_array)\n        image = Image.fromarray(image_array)\n    plt.savefig(os.path.join(base_path,\"out/SR\")+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n    plt.show()\n    plt.close()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change the batch-size based on your system memory\n\nepochs=5000\nbatch_size=8","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n# base_path =\"/home/aiteam/TeamData/Vishal_Data/Learning\" \nbase_path = os.getcwd()\n\n#lr_path = os.path.join(base_path,\"trainImages\")\nhr_path =celebData\n#valid_path = os.path.join(base_path,\"SR_valid\")\nweight_file = os.path.join(base_path,\"SRPT_weights\")\nout_path = os.path.join(base_path,\"out\")\n\nif not os.path.exists(weight_file):\n    os.makedirs(weight_file)\n\nif not os.path.exists(out_path):\n    os.makedirs(out_path)\n\n    \n#LR_images_list = os.listdir(lr_path)\nHR_images_list = imageList\nbatch_count = len(HR_images_list)//batch_size\nbatch_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Starting of training and defining losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"#batch_count=60\nfor epoch in range(epochs):\n    d1loss_list=[]\n    d2loss_list=[]\n    gloss_list=[]\n    vloss_list=[]\n    mloss_list=[]\n    \n    for batch in tqdm(range(batch_count)):\n        hr_imagesList = [img for img in HR_images_list[batch*batch_size:(batch+1)*batch_size]]\n        lr_images = loadLRImages(hr_imagesList,hr_path)/255\n        hr_images = loadImages(hr_imagesList,hr_path,True)/255\n        \n        \n        disc.zero_grad()\n        gen_out = gen(torch.from_numpy(lr_images).to(cuda).float())\n        _,f_label = disc(gen_out)\n        _,r_label = disc(torch.from_numpy(hr_images).to(cuda).float())\n        d1_loss = torch.mean(disc_loss(f_label,torch.zeros_like(f_label,dtype=torch.float)))\n        d2_loss = torch.mean(disc_loss(r_label,torch.ones_like(r_label,dtype=torch.float)))\n        d_loss = d1_loss+d2_loss\n        d2_loss.backward()\n        d1_loss.backward(retain_graph=True)\n        \n#         d_loss.backward(retain_graph=True)\n        disc_optimizer.step()\n        \n        \n        gen.zero_grad()\n        g_loss = gen_loss(f_label,torch.ones_like(f_label,dtype=torch.float))\n        v_loss = vgg_loss(vgg.features[:7](gen_out),vgg.features[:7](torch.from_numpy(hr_images).to(cuda).float()))\n        m_loss = mse_loss(gen_out,torch.from_numpy(hr_images).to(cuda).float())\n        generator_loss = torch.sum(g_loss+v_loss+m_loss)\n        generator_loss.backward()\n        gen_optimizer.step()\n        \n        d1loss_list.append(d1_loss.item())\n        d2loss_list.append(d2_loss.item())\n        \n        gloss_list.append(g_loss.item())\n        vloss_list.append(v_loss.item())\n        mloss_list.append(m_loss.item())\n        \n        \n        \n#         print(\"d1Loss ::: \"+str((d1_loss.item()))+\" d2Loss ::: \"+str((d2_loss.item())))\n#         print(\"gloss ::: \"+str((g_loss.item()))+\" vloss ::: \"+str((v_loss.item()))+\" mloss ::: \"+str((m_loss.item())))\n    print(\"Epoch ::::  \"+str(epoch+1)+\"  d1_loss ::: \"+str(np.mean(d1loss_list))+\"  d2_loss :::\"+str(np.mean(d2loss_list)))\n    print(\"genLoss ::: \"+str(np.mean(gloss_list))+\"  vggLoss ::: \"+str(np.mean(vloss_list))+\"  MeanLoss  ::: \"+str(np.mean(mloss_list)))\n    \n    if(epoch%3==0):\n        \n        checkpoint = {'model': Generator(),\n              'input_size': 64,\n              'output_size': 256,\n              'state_dict': gen.state_dict()}\n        torch.save(checkpoint,os.path.join(weight_file,\"SR\"+str(epoch+1)+\".pth\"))\n        torch.cuda.empty_cache()\n        \n        out_images = imagePostProcess(images[-2:],os.path.join(weight_file,\"SR\"+str(epoch+1)+\".pth\"))\n#         print(out_images.shape)\n#         test_images = loadLRImages(images[:-3],hr_path)/255\n#         test_images = np.reshape(test_images,(test_images[0],test_images.shape[3],test_images.shape[1],test_images.shape[2]))\n#         out_images = gen(torch.from_numpy(test_images).to(cuda).float())\n#         out_images = np.reshape(out_images,(out_images[0],out_images[2],out_images[3],out_images[1]))\n        show_samples(out_images)\n#     break\n        ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}